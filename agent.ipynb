{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d75e9aa-448b-490b-a101-940dee47a2db",
   "metadata": {},
   "source": [
    "# Actor Critic Agent(s) for Resource Allocation\n",
    "- Tests with Gym first\n",
    "- https://pytorch.org/docs/stable/tensorboard.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3eeca8d2-bb5d-47a6-8178-c2ff22bde6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import gym.spaces as spaces\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e82e3-2d8e-498a-b882-8b127807bd9a",
   "metadata": {},
   "source": [
    "## Actor-Critic\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71e9cf87-0919-4ffe-9d96-3b981a43d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_UNITS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "26c260a1-badf-4b1d-a899-6c1223b27000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalWrapper(torch.distributions.Categorical):\n",
    "    def log_probs(self, actions):\n",
    "        return super.log_prob(actions.squeeze(-1)).view(actions.size(0), -1).sum(-1).unsqueeze(-1)\n",
    "    \n",
    "    def sample(self):\n",
    "        return super().sample().unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1bc04ae9-0403-4ba5-a05c-1a40015e1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_shape, action_space, discrete=True):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self._is_discrete = discrete\n",
    "        self._num_inputs = obs_shape[0]\n",
    "        self._n_hidden = NUM_HIDDEN_UNITS\n",
    "        if discrete:\n",
    "            self._num_outputs = action_space.n\n",
    "        else:\n",
    "            self._num_outputs = action_space.shape[0]\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self._num_inputs, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, self._num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self._num_inputs, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, 1)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward through networks\n",
    "        actions = self.actor(x)\n",
    "        critique = self.critic(x)\n",
    "        \n",
    "        return actions, critique\n",
    "    \n",
    "    def act(self, x):\n",
    "        actions, value = self(x)\n",
    "        \n",
    "        # sample\n",
    "        dist = CategoricalWrapper(logits=actions)\n",
    "        action = self.dist.sample()\n",
    "        \n",
    "        # generate log probabilities\n",
    "        log_prob = dist.log_probs(action)\n",
    "        return value, action, log_prob\n",
    "        \n",
    "    def evaluate_action(self, x, action):\n",
    "        actions, value = self(x)\n",
    "        \n",
    "        dist = CategoricalWrapper(logits=actions)\n",
    "        \n",
    "        # generate log probabilities\n",
    "        log_prob = dist.log_probs(action)\n",
    "        return value, log_prob\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        _, value = self(x)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ecfd9187-d996-4469-9130-4d12c67a048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Ikostrikov's A2C Rollout Storage\n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, obs_shape, action_space, num_processes=1):\n",
    "        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        \n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        \n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        \n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            action_shape = 1\n",
    "        else:\n",
    "            action_shape = action_space.shape[0]\n",
    "        \n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "        \n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long()\n",
    "        \n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        # Masks that indicate whether it's a true terminal state\n",
    "        # or time limit end state\n",
    "        # self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.step = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.obs = self.obs.to(device)\n",
    "        \n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.value_preds = self.value_preds.to(device)\n",
    "        self.returns = self.returns.to(device)\n",
    "        \n",
    "        self.action_log_probs = self.action_log_probs.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "\n",
    "    def insert(self, obs, actions, action_log_probs, value_preds, rewards, masks, bad_masks):\n",
    "        self.obs[self.step + 1].copy_(obs)\n",
    "        \n",
    "        self.actions[self.step].copy_(actions)\n",
    "        self.action_log_probs[self.step].copy_(action_log_probs)\n",
    "        self.value_preds[self.step].copy_(value_preds)\n",
    "        \n",
    "        self.rewards[self.step].copy_(rewards)\n",
    "        self.masks[self.step + 1].copy_(masks)\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def after_update(self):\n",
    "        self.obs[0].copy_(self.obs[-1])\n",
    "        self.recurrent_hidden_states[0].copy_(self.recurrent_hidden_states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value, gamma):\n",
    "        self.returns[-1] = next_value\n",
    "        \n",
    "        for step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[step] = (self.returns[step + 1] * \\\n",
    "                gamma * self.masks[step + 1] + self.rewards[step])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ba8ab8d2-e92f-4f48-8218-80b61d998c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, lr=7e-4, eps=1e-5, alpha=0.99):\n",
    "        self.optimizer = optim.RMSprop(model.parameters(), lr, eps=eps, alpha=alpha)\n",
    "    \n",
    "    # adapted from Ikostrikov's A2C\n",
    "    def update(self, rollouts):\n",
    "        obs_shape = rollouts.obs.size()[2:]\n",
    "        action_shape = rollouts.actions.size()[-1]\n",
    "        num_steps, num_processes, _ = rollouts.rewards.size()\n",
    "        \n",
    "        values, action_log_probs, dist_entropy, _ = self.actor_critic.evaluate_actions(\n",
    "            rollouts.obs[:-1].view(-1, *obs_shape),\n",
    "            rollouts.recurrent_hidden_states[0].view(\n",
    "                -1, self.actor_critic.recurrent_hidden_state_size),\n",
    "            rollouts.masks[:-1].view(-1, 1),\n",
    "            rollouts.actions.view(-1, action_shape)\n",
    "        )\n",
    "\n",
    "        values = values.view(num_steps, num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        advantages = rollouts.returns[:-1] - values\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(advantages.detach() * action_log_probs).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        (value_loss + action_loss).backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return value_loss.item(), action_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4389d24a-f4bd-40be-b6fd-47c57b6c3e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCritic(\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=32, out_features=5, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a = ActorCritic((3, 1), spaces.Discrete(5))\n",
    "print(a)\n",
    "o = Optimizer(a, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f0369-a487-4d5c-a511-6e4c208e5f32",
   "metadata": {},
   "source": [
    "## Gym Environment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79963f87-2a4f-4380-9035-4d1594f67fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4280bf2-2f63-49f6-a1ca-28e704cc52f5",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4eb2fb9-ca5f-486c-8bc1-de6ad1c74277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SustainableEnvironment(gym.Env):\n",
    "    def __init__(self, max_land_resources = 100):\n",
    "        self.action_space = spaces.Discrete(11)\n",
    "        self.max_land_resources = max_land_resources\n",
    "        self.high = np.array(\n",
    "            [\n",
    "                2*self.max_land_resources,\n",
    "                self.max_land_resources\n",
    "            ],\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        self.low = np.array(\n",
    "            [\n",
    "                0,\n",
    "                0\n",
    "            ],\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.int32)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = [random.randint(2, 2*self.max_land_resources), random.randint(0, self.max_land_resources)]\n",
    "        return np.array(self.state, dtype=np.int32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        percent_land_use = action/10\n",
    "        land_utilized = int(self.state[1] * percent_land_use)\n",
    "        self.state[1] = self.state[1] - land_utilized\n",
    "        population_left_to_feed = max(0, self.state[0] - land_utilized)\n",
    "        pass\n",
    "    \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "963b5ce6-0c2d-4a0f-89a7-b9bcf7d953fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112  90]\n"
     ]
    }
   ],
   "source": [
    "env = SustainableEnvironment()\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9389aca1-eb8f-40f4-bd70-5211f461921b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
