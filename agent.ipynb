{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d75e9aa-448b-490b-a101-940dee47a2db",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Actor Critic Agent(s) for Resource Allocation\n",
    "- Tests with Gym first\n",
    "- https://pytorch.org/docs/stable/tensorboard.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "3eeca8d2-bb5d-47a6-8178-c2ff22bde6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import gym.spaces as spaces\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e82e3-2d8e-498a-b882-8b127807bd9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Actor-Critic\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e9cf87-0919-4ffe-9d96-3b981a43d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HIDDEN_UNITS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c260a1-badf-4b1d-a899-6c1223b27000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalWrapper(torch.distributions.Categorical):\n",
    "    def log_probs(self, actions):\n",
    "        return super().log_prob(actions.squeeze(-1)).view(actions.size(0), -1).sum(-1).unsqueeze(-1)\n",
    "    \n",
    "    def sample(self):\n",
    "        return super().sample().unsqueeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc04ae9-0403-4ba5-a05c-1a40015e1daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_shape, action_space, discrete=True):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self._is_discrete = discrete\n",
    "        self._num_inputs = obs_shape[0]\n",
    "        self._n_hidden = NUM_HIDDEN_UNITS\n",
    "        if discrete:\n",
    "            self._num_outputs = action_space.n\n",
    "        else:\n",
    "            self._num_outputs = action_space.shape[0]\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(self._num_inputs, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, self._num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self._num_inputs, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, self._n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self._n_hidden, 1)\n",
    "        )\n",
    "        \n",
    "        self.train()        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # forward through networks\n",
    "        actions = self.actor(x)\n",
    "        critique = self.critic(x)\n",
    "        \n",
    "        return actions, critique\n",
    "    \n",
    "    def act(self, x):\n",
    "        actions, value = self(x)\n",
    "        \n",
    "        # sample\n",
    "        dist = CategoricalWrapper(logits=actions)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # generate log probabilities\n",
    "        log_prob = dist.log_probs(action)\n",
    "        return value, action, log_prob\n",
    "        \n",
    "    def evaluate_action(self, x, action):\n",
    "        actions, value = self(x)\n",
    "        \n",
    "        dist = CategoricalWrapper(logits=actions)\n",
    "        \n",
    "        # generate log probabilities\n",
    "        log_prob = dist.log_probs(action)\n",
    "        return value, log_prob\n",
    "    \n",
    "    def get_value(self, x):\n",
    "        _, value = self(x)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd9187-d996-4469-9130-4d12c67a048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from Ikostrikov's A2C Rollout Storage\n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, obs_shape, action_space, num_processes=1):\n",
    "        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n",
    "        \n",
    "        self.rewards = torch.zeros(num_steps, num_processes, 1)\n",
    "        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        \n",
    "        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n",
    "        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n",
    "        \n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            action_shape = 1\n",
    "        else:\n",
    "            action_shape = action_space.shape[0]\n",
    "        \n",
    "        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n",
    "        \n",
    "        if action_space.__class__.__name__ == 'Discrete':\n",
    "            self.actions = self.actions.long()\n",
    "        \n",
    "        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        # Masks that indicate whether it's a true terminal state\n",
    "        # or time limit end state\n",
    "        # self.bad_masks = torch.ones(num_steps + 1, num_processes, 1)\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.step = 0\n",
    "\n",
    "    def to(self, device):\n",
    "        self.obs = self.obs.to(device)\n",
    "        \n",
    "        self.rewards = self.rewards.to(device)\n",
    "        self.value_preds = self.value_preds.to(device)\n",
    "        self.returns = self.returns.to(device)\n",
    "        \n",
    "        self.action_log_probs = self.action_log_probs.to(device)\n",
    "        self.actions = self.actions.to(device)\n",
    "        self.masks = self.masks.to(device)\n",
    "\n",
    "    def insert(self, obs, actions, action_log_probs, value_preds, rewards, masks):\n",
    "        self.obs[self.step + 1].copy_(obs)\n",
    "        \n",
    "        self.actions[self.step].copy_(actions)\n",
    "        self.action_log_probs[self.step].copy_(action_log_probs)\n",
    "        self.value_preds[self.step].copy_(value_preds)\n",
    "        \n",
    "        self.rewards[self.step].copy_(rewards)\n",
    "        self.masks[self.step + 1].copy_(masks)\n",
    "\n",
    "        self.step = (self.step + 1) % self.num_steps\n",
    "\n",
    "    def after_update(self):\n",
    "        self.obs[0].copy_(self.obs[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "\n",
    "    def compute_returns(self, next_value, gamma):\n",
    "        self.returns[-1] = next_value\n",
    "        \n",
    "        for step in reversed(range(self.rewards.size(0))):\n",
    "            self.returns[step] = (self.returns[step + 1] * \\\n",
    "                gamma * self.masks[step + 1] + self.rewards[step])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ab8d2-e92f-4f48-8218-80b61d998c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, lr=7e-4, eps=1e-5, alpha=0.99):\n",
    "        self.optimizer = optim.RMSprop(model.parameters(), lr, eps=eps, alpha=alpha)\n",
    "        self.model = model\n",
    "    \n",
    "    # adapted from Ikostrikov's A2C\n",
    "    def update(self, rollouts):\n",
    "        obs_shape = rollouts.obs.size()[2:]\n",
    "        action_shape = rollouts.actions.size()[-1]\n",
    "        num_steps, num_processes, _ = rollouts.rewards.size()\n",
    "        \n",
    "        values, action_log_probs = self.model.evaluate_action(\n",
    "            rollouts.obs[:-1].view(-1, *obs_shape),\n",
    "            rollouts.actions.view(-1, action_shape)\n",
    "        )\n",
    "\n",
    "        values = values.view(num_steps, num_processes, 1)\n",
    "        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n",
    "\n",
    "        advantages = rollouts.returns[:-1] - values\n",
    "        value_loss = advantages.pow(2).mean()\n",
    "\n",
    "        action_loss = -(advantages.detach() * action_log_probs).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        (value_loss + action_loss).backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return value_loss.item(), action_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389d24a-f4bd-40be-b6fd-47c57b6c3e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCritic(\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=32, out_features=5, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "a = ActorCritic((3, 1), spaces.Discrete(5))\n",
    "print(a)\n",
    "o = Optimizer(a, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f0369-a487-4d5c-a511-6e4c208e5f32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Gym Environment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "d583ffe6-002a-44a3-910d-551b2b3bfd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "print(device)\n",
    "\n",
    "agent = ActorCritic(env.observation_space.shape, env.action_space)\n",
    "agent.to(device)\n",
    "\n",
    "opt = Optimizer(agent)\n",
    "\n",
    "rollouts = RolloutStorage(5, env.observation_space.shape, env.action_space)\n",
    "rollouts.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79963f87-2a4f-4380-9035-4d1594f67fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 0 : mean reward of 10.0\n",
      "At episode 100 : mean reward of 17.3\n",
      "At episode 200 : mean reward of 11.9\n",
      "At episode 300 : mean reward of 11.6\n",
      "At episode 400 : mean reward of 15.7\n",
      "At episode 500 : mean reward of 16.0\n",
      "At episode 600 : mean reward of 9.9\n",
      "At episode 700 : mean reward of 13.3\n",
      "At episode 800 : mean reward of 11.7\n",
      "At episode 900 : mean reward of 10.0\n",
      "At episode 1000 : mean reward of 10.0\n",
      "At episode 1100 : mean reward of 11.4\n",
      "At episode 1200 : mean reward of 9.7\n",
      "At episode 1300 : mean reward of 11.6\n",
      "At episode 1400 : mean reward of 9.7\n",
      "At episode 1500 : mean reward of 13.3\n",
      "At episode 1600 : mean reward of 9.5\n",
      "At episode 1700 : mean reward of 14.7\n",
      "At episode 1800 : mean reward of 9.7\n",
      "At episode 1900 : mean reward of 9.9\n",
      "At episode 2000 : mean reward of 12.2\n",
      "At episode 2100 : mean reward of 9.4\n",
      "At episode 2200 : mean reward of 9.5\n",
      "At episode 2300 : mean reward of 16.2\n",
      "At episode 2400 : mean reward of 11.5\n",
      "At episode 2500 : mean reward of 9.8\n",
      "At episode 2600 : mean reward of 12.7\n",
      "At episode 2700 : mean reward of 11.8\n",
      "At episode 2800 : mean reward of 11.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [232], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m         next_value \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_value(rollouts\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     39\u001b[0m     rollouts\u001b[38;5;241m.\u001b[39mcompute_returns(next_value, \u001b[38;5;241m0.99\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     value_loss, action_loss \u001b[38;5;241m=\u001b[39m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrollouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     rollouts\u001b[38;5;241m.\u001b[39mafter_update()\n\u001b[1;32m     43\u001b[0m cur_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn [226], line 25\u001b[0m, in \u001b[0;36mOptimizer.update\u001b[0;34m(self, rollouts)\u001b[0m\n\u001b[1;32m     23\u001b[0m action_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(advantages\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m*\u001b[39m action_log_probs)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 25\u001b[0m \u001b[43m(\u001b[49m\u001b[43mvalue_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maction_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Desktop/ubhack/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ubhack/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "history = []\n",
    "M = 1\n",
    "\n",
    "first_time = True\n",
    "\n",
    "obs = env.reset()\n",
    "rollouts.obs[0].copy_(torch.from_numpy(obs))\n",
    "rollouts.to(device)\n",
    "\n",
    "cur_step = 0\n",
    "\n",
    "for i in range(5000):\n",
    "    if not first_time:\n",
    "        obs = env.reset()\n",
    "    else:\n",
    "        first_time = False\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    n = 1\n",
    "\n",
    "    while not done:\n",
    "        value, action, log_prob = agent.act(rollouts.obs[cur_step])\n",
    "        next_obs, reward, done, _ = env.step(action.item())\n",
    "        next_obs = torch.from_numpy(next_obs)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        masks = torch.FloatTensor([[0.0] if done else [1.0]])\n",
    "        \n",
    "        rollouts.insert(next_obs.clone(), action, log_prob, value, reward, masks.clone())\n",
    "        \n",
    "        if cur_step == 5:\n",
    "            cur_step = 0\n",
    "            with torch.no_grad():\n",
    "                next_value = agent.get_value(rollouts.obs[-1]).detach()\n",
    "                \n",
    "            rollouts.compute_returns(next_value, 0.99)\n",
    "            value_loss, action_loss = opt.update(rollouts)\n",
    "            rollouts.after_update()\n",
    "        \n",
    "        cur_step += 1\n",
    "        \n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "    history.append(total_reward)\n",
    "    # self.episodes.append(i)\n",
    "    mean_time = np.mean(history[-10:])\n",
    "    # self.ep_decays.append(self.epsilon)\n",
    "    # self.epsilon = max(0.001, self.epsilon*self.ep_decay)\n",
    "   #  self.total_reward_ep.append(total_reward)\n",
    "    if i % 100 == 0:\n",
    "        print('At episode', i, ': mean reward of', mean_time)\n",
    "    if mean_time > 480.:\n",
    "        print('Optimal Agent Achieved')\n",
    "        break\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854e077b-57ae-4355-b7ce-612cbe49ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 39.5     |\n",
      "|    ep_rew_mean        | 39.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5711     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.61    |\n",
      "|    explained_variance | 0.155    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.58     |\n",
      "|    value_loss         | 8.04     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 58.3     |\n",
      "|    ep_rew_mean        | 58.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5972     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.641   |\n",
      "|    explained_variance | 0.292    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.18     |\n",
      "|    value_loss         | 6.49     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 79.4     |\n",
      "|    ep_rew_mean        | 79.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 6052     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.63    |\n",
      "|    explained_variance | 0.00277  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.11     |\n",
      "|    value_loss         | 6.06     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 97.5     |\n",
      "|    ep_rew_mean        | 97.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 6095     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.571   |\n",
      "|    explained_variance | 0.0103   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.929    |\n",
      "|    value_loss         | 5.29     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 104      |\n",
      "|    ep_rew_mean        | 104      |\n",
      "| time/                 |          |\n",
      "|    fps                | 6123     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.514   |\n",
      "|    explained_variance | 0.0155   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 1.03     |\n",
      "|    value_loss         | 4.73     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 117      |\n",
      "|    ep_rew_mean        | 117      |\n",
      "| time/                 |          |\n",
      "|    fps                | 6142     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.554   |\n",
      "|    explained_variance | -0.00113 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.99     |\n",
      "|    value_loss         | 4.11     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 134      |\n",
      "|    ep_rew_mean        | 134      |\n",
      "| time/                 |          |\n",
      "|    fps                | 6152     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.59    |\n",
      "|    explained_variance | 0.000135 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.913    |\n",
      "|    value_loss         | 3.59     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 153       |\n",
      "|    ep_rew_mean        | 153       |\n",
      "| time/                 |           |\n",
      "|    fps                | 6164      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.539    |\n",
      "|    explained_variance | -0.000542 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 0.836     |\n",
      "|    value_loss         | 3.07      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 171      |\n",
      "|    ep_rew_mean        | 171      |\n",
      "| time/                 |          |\n",
      "|    fps                | 6175     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.535   |\n",
      "|    explained_variance | 0.000262 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.663    |\n",
      "|    value_loss         | 2.69     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 185       |\n",
      "|    ep_rew_mean        | 185       |\n",
      "| time/                 |           |\n",
      "|    fps                | 6187      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.526    |\n",
      "|    explained_variance | -0.000666 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.458     |\n",
      "|    value_loss         | 2.29      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 200      |\n",
      "|    ep_rew_mean        | 200      |\n",
      "| time/                 |          |\n",
      "|    fps                | 6194     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.552   |\n",
      "|    explained_variance | 0.021    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.612    |\n",
      "|    value_loss         | 1.9      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 213      |\n",
      "|    ep_rew_mean        | 213      |\n",
      "| time/                 |          |\n",
      "|    fps                | 6199     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.558   |\n",
      "|    explained_variance | 0.000663 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.493    |\n",
      "|    value_loss         | 1.6      |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4280bf2-2f63-49f6-a1ca-28e704cc52f5",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "d4eb2fb9-ca5f-486c-8bc1-de6ad1c74277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SustainableEnvironment(gym.Env):\n",
    "    def __init__(self, max_land_resources = 100, max_food_storage = 200, disaster_probability = 0.05):\n",
    "        self.action_space = spaces.Discrete(11)\n",
    "        self.max_land_resources = max_land_resources\n",
    "        self.max_food_storage = max_food_storage\n",
    "        self.disaster_probability = disaster_probability\n",
    "        self.high = np.array(\n",
    "            [\n",
    "                2*self.max_land_resources,\n",
    "                self.max_land_resources,\n",
    "                self.max_food_storage\n",
    "            ],\n",
    "            dtype=np.int32,\n",
    "        )\n",
    "        self.low = np.array(\n",
    "            [\n",
    "                0,\n",
    "                0,\n",
    "                0\n",
    "            ],\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(self.low, self.high, dtype=np.int32)\n",
    "        self.food_storage = 0\n",
    "        self.resource_recovery = {\n",
    "            0:100,\n",
    "            1:95,\n",
    "            2:90,\n",
    "            3:85,\n",
    "            4:80,\n",
    "            5:75,\n",
    "            6:70,\n",
    "            7:65,\n",
    "            8:60,\n",
    "            9:55,\n",
    "            10:50\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        # self.state = [random.randint(11, 2*self.max_land_resources), random.randint(11, self.max_land_resources), random.randint(0, self.max_food_storage)]\n",
    "        self.state = [50, 100, 0]\n",
    "        return np.array(self.state, dtype=np.int32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        population, land_resources, food_storage = self.state\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        percent_land_use = action/10\n",
    "        land_utilized = food_produced =  int(land_resources * percent_land_use)\n",
    "        # print(f'land_utilized: {land_utilized}')\n",
    "        land_resources = min(self.max_land_resources, (land_resources - land_utilized) + (land_resources - land_utilized) * self.resource_recovery[action]/100)\n",
    "        # print(f'land_resources: {land_resources}')\n",
    "        # Dividng the land resources thing. More land, more utilization, less recovery. More land, less utilization, more recovery.\n",
    "        # Less land, more utilization, \n",
    "        people_died = 0\n",
    "        if(population > food_produced):\n",
    "            if(population > food_storage + food_produced):\n",
    "                # Some people will die. Negative reward should be given.\n",
    "                people_died = population - food_storage - food_produced\n",
    "                # print(f'people_died_hunger: {people_died}')\n",
    "                population = food_storage + food_produced\n",
    "                # print(f'new population for now: {population}')\n",
    "                food_storage = 0\n",
    "                reward -= people_died\n",
    "            else:\n",
    "                # No one should die of hunger. Some food will be taken from fresh produce\n",
    "                # and some will be taken from storage. Population remains the same for now.\n",
    "                food_storage -= (population - food_produced)\n",
    "                # print(f'food_storage: {food_storage}')\n",
    "        else:\n",
    "            # No one should die. All the demand will be satisfied by fresh produce.\n",
    "            # Leftover food will be added to storage. Population remains the same for now.\n",
    "            food_storage += (food_produced - population)\n",
    "            # print(f'food_storage: {food_storage}')\n",
    "        food_storage_overflow = max(0, food_storage - self.max_food_storage)\n",
    "        # print(f'food_storage_overflow: {food_storage_overflow}')\n",
    "        reward -= food_storage_overflow\n",
    "        # Negative reward should be given if overflow occurs, because such behavior is discouraged.\n",
    "        food_storage = min(self.max_food_storage, food_storage)\n",
    "        # print(f'food_storage: {food_storage}')\n",
    "        # How much does each of the state variables decrease? Two options: either naturally, which is 5%-10%, or due to a disaster, \n",
    "        # which will be 50% to 70% for food storage, 30% to 50% for population and 10% to 20% for land.\n",
    "        disaster = np.random.choice(2, p = [self.disaster_probability, 1 - self.disaster_probability]) == 0\n",
    "        # print(disaster)\n",
    "        if(disaster):\n",
    "            # Disaster occured this year\n",
    "            # Decreasing all the state values first\n",
    "            percent_food_spoiled = random.uniform(0.5, 0.7)\n",
    "            # print(f'percent_food_spoiled: {percent_food_spoiled}')\n",
    "            food_decrease = int(round(food_storage * percent_food_spoiled))\n",
    "            # print(f'food_decrease: {food_decrease}')\n",
    "            population_decrease_factor = random.uniform(0.3, 0.5)\n",
    "            # print(f'population_decrease_factor: {population_decrease_factor}')\n",
    "            population_decrease = int(round(population * population_decrease_factor))\n",
    "            # print(f'population_decrease: {population_decrease}')\n",
    "            land_decrease_factor = random.uniform(0.1, 0.2)\n",
    "            # print(f'land_decrease_factor: {land_decrease_factor}')\n",
    "            land_decrease = int(round(land_resources * land_decrease_factor))\n",
    "            # print(f'land_decrease: {land_decrease}')\n",
    "            \n",
    "            # Now increasing values for disaster year. Assuming population will not increase by a lot in a disaster.\n",
    "            population_increase_factor = random.uniform(0.01, 0.20)\n",
    "            # print(f'population_increase_factor: {population_increase_factor}')\n",
    "            population_increase = int(round(population * population_increase_factor))\n",
    "            # print(f'population_increase: {population_increase}')\n",
    "            \n",
    "        else:\n",
    "            # Disaster did not occur this year. Land does not deteriorate over time, it only gets better for farming.\n",
    "            percent_food_spoiled = random.uniform(0.05, 0.10)\n",
    "            # print(f'percent_food_spoiled: {percent_food_spoiled}')\n",
    "            food_decrease = int(round(food_storage * percent_food_spoiled))\n",
    "            # print(f'food_decrease: {food_decrease}')\n",
    "            population_decrease_factor = random.uniform(0.05, 0.10)\n",
    "            # print(f'population_decrease_factor: {population_decrease_factor}')\n",
    "            population_decrease = int(round(population * population_decrease_factor))\n",
    "            # print(f'population_decrease: {population_decrease}')\n",
    "            land_decrease_factor = 0\n",
    "            # print(f'land_decrease_factor: {land_decrease_factor}')\n",
    "            land_decrease = int(land_resources * land_decrease_factor)\n",
    "            \n",
    "            # Increasing state values according to if disaster did not occur.\n",
    "            population_increase_factor = random.uniform(0.05, 0.20)\n",
    "            # print(f'population_increase_factor: {population_increase_factor}')\n",
    "            population_increase = int(round(population * population_increase_factor))\n",
    "            # print(f'population_increase: {population_increase}')\n",
    "            \n",
    "        population = max(0, population - population_decrease + population_increase)\n",
    "        # print(f'population: {population}')\n",
    "        food_storage = max(0, food_storage - food_decrease)\n",
    "        # print(f'food_storage: {food_storage}')\n",
    "        land_resources = max(0, land_resources - land_decrease)\n",
    "        # print(f'land_resources: {land_resources}')\n",
    "        \n",
    "        if(population < 10 or land_resources < 10):\n",
    "            done = True\n",
    "        if(people_died == 0 and food_storage_overflow == 0):\n",
    "            reward += population + land_resources\n",
    "        \n",
    "        \n",
    "        self.state = np.array([population, land_resources, food_storage], dtype=np.int32)\n",
    "        return self.state, reward, done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "963b5ce6-0c2d-4a0f-89a7-b9bcf7d953fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50 100   0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 11, 100,   0], dtype=int32), -40, False, {})"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = SustainableEnvironment()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "# while not done:\n",
    "#     print(f'Observation: {obs}')\n",
    "#     action = np.random.choice(11)\n",
    "#     print(f'Action: {action}')\n",
    "#     new_obs, reward, done = env.step(action)\n",
    "#     print(f'New Obs: {new_obs}')\n",
    "#     obs = new_obs\n",
    "print(obs)\n",
    "env.step(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389aca1-eb8f-40f4-bd70-5211f461921b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9e518ad2-ddd7-4a09-9a21-c56d5a55c473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "7247d042-c2fd-48a3-baa8-2cd1fbd19a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ecbb13cc-479e-4e79-8bbb-76bb7c97c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "# Example for the CartPole environment\n",
    "register(\n",
    "    # unique identifier for the env `name-version`\n",
    "    id=\"SustainableEnvironment-v1\",\n",
    "    # path to the class for creating the env\n",
    "    # Note: entry_point also accept a class as input (and not only a string)\n",
    "    entry_point=SustainableEnvironment,\n",
    "    # Max number of steps per episode, using a `TimeLimitWrapper`\n",
    "    max_episode_steps=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fc1b32dd-4e19-4b96-83e7-fba974ba3a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 8.38     |\n",
      "|    ep_rew_mean        | 410      |\n",
      "| time/                 |          |\n",
      "|    fps                | 5017     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | 0.0206   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 72.2     |\n",
      "|    value_loss         | 2.01e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 15.1     |\n",
      "|    ep_rew_mean        | 927      |\n",
      "| time/                 |          |\n",
      "|    fps                | 5135     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0.00968  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 428      |\n",
      "|    value_loss         | 6.72e+04 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 26.8     |\n",
      "|    ep_rew_mean        | 2.05e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 5182     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.8     |\n",
      "|    explained_variance | 1.47e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 518      |\n",
      "|    value_loss         | 1.34e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 41.7      |\n",
      "|    ep_rew_mean        | 3.72e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 5209      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.24     |\n",
      "|    explained_variance | -3.81e-06 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 335       |\n",
      "|    value_loss         | 1.46e+05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 54.4      |\n",
      "|    ep_rew_mean        | 5.14e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 5223      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.13     |\n",
      "|    explained_variance | -8.34e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 471       |\n",
      "|    value_loss         | 1.11e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 73.8     |\n",
      "|    ep_rew_mean        | 7.22e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 5235     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.09    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 400      |\n",
      "|    value_loss         | 1.69e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 90.9      |\n",
      "|    ep_rew_mean        | 9.12e+03  |\n",
      "| time/                 |           |\n",
      "|    fps                | 5241      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.17     |\n",
      "|    explained_variance | -9.54e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 364       |\n",
      "|    value_loss         | 1.34e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 107      |\n",
      "|    ep_rew_mean        | 1.08e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 5250     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.11    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 373      |\n",
      "|    value_loss         | 1.34e+05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 123      |\n",
      "|    ep_rew_mean        | 1.27e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 5255     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 341      |\n",
      "|    value_loss         | 1.24e+05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 138       |\n",
      "|    ep_rew_mean        | 1.45e+04  |\n",
      "| time/                 |           |\n",
      "|    fps                | 5261      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.12     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 374       |\n",
      "|    value_loss         | 1.53e+05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 154      |\n",
      "|    ep_rew_mean        | 1.62e+04 |\n",
      "| time/                 |          |\n",
      "|    fps                | 5266     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.942   |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 256      |\n",
      "|    value_loss         | 1.2e+05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 171      |\n",
      "|    ep_rew_mean        | 1.8e+04  |\n",
      "| time/                 |          |\n",
      "|    fps                | 5268     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.03    |\n",
      "|    explained_variance | 7.75e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 373      |\n",
      "|    value_loss         | 6.66e+04 |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "env = make_vec_env(\"SustainableEnvironment-v1\", n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"a2c_sustainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6a93b-83db-4094-b745-7b9d36827d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
